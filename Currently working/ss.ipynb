{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL IE 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevent libarbries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris Dataset:-\n",
    "1. 1st column- Sepal Length\n",
    "2. 2nd column- Sepal Width\n",
    "3. 3rd column- Petal Length\n",
    "4. 4th column- Petal Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  + target\n",
      "[5.1 3.5 1.4 0.2] ->  0\n",
      "[4.9 3.  1.4 0.2] ->  0\n",
      "[4.7 3.2 1.3 0.2] ->  0\n",
      "[4.6 3.1 1.5 0.2] ->  0\n",
      "[5.  3.6 1.4 0.2] ->  0\n",
      "[5.4 3.9 1.7 0.4] ->  0\n",
      "[4.6 3.4 1.4 0.3] ->  0\n",
      "[5.  3.4 1.5 0.2] ->  0\n",
      "[4.4 2.9 1.4 0.2] ->  0\n",
      "[4.9 3.1 1.5 0.1] ->  0\n",
      "[5.4 3.7 1.5 0.2] ->  0\n",
      "[4.8 3.4 1.6 0.2] ->  0\n",
      "[4.8 3.  1.4 0.1] ->  0\n",
      "[4.3 3.  1.1 0.1] ->  0\n",
      "[5.8 4.  1.2 0.2] ->  0\n",
      "[5.7 4.4 1.5 0.4] ->  0\n",
      "[5.4 3.9 1.3 0.4] ->  0\n",
      "[5.1 3.5 1.4 0.3] ->  0\n",
      "[5.7 3.8 1.7 0.3] ->  0\n",
      "[5.1 3.8 1.5 0.3] ->  0\n",
      "[5.4 3.4 1.7 0.2] ->  0\n",
      "[5.1 3.7 1.5 0.4] ->  0\n",
      "[4.6 3.6 1.  0.2] ->  0\n",
      "[5.1 3.3 1.7 0.5] ->  0\n",
      "[4.8 3.4 1.9 0.2] ->  0\n",
      "[5.  3.  1.6 0.2] ->  0\n",
      "[5.  3.4 1.6 0.4] ->  0\n",
      "[5.2 3.5 1.5 0.2] ->  0\n",
      "[5.2 3.4 1.4 0.2] ->  0\n",
      "[4.7 3.2 1.6 0.2] ->  0\n",
      "[4.8 3.1 1.6 0.2] ->  0\n",
      "[5.4 3.4 1.5 0.4] ->  0\n",
      "[5.2 4.1 1.5 0.1] ->  0\n",
      "[5.5 4.2 1.4 0.2] ->  0\n",
      "[4.9 3.1 1.5 0.2] ->  0\n",
      "[5.  3.2 1.2 0.2] ->  0\n",
      "[5.5 3.5 1.3 0.2] ->  0\n",
      "[4.9 3.6 1.4 0.1] ->  0\n",
      "[4.4 3.  1.3 0.2] ->  0\n",
      "[5.1 3.4 1.5 0.2] ->  0\n",
      "[5.  3.5 1.3 0.3] ->  0\n",
      "[4.5 2.3 1.3 0.3] ->  0\n",
      "[4.4 3.2 1.3 0.2] ->  0\n",
      "[5.  3.5 1.6 0.6] ->  0\n",
      "[5.1 3.8 1.9 0.4] ->  0\n",
      "[4.8 3.  1.4 0.3] ->  0\n",
      "[5.1 3.8 1.6 0.2] ->  0\n",
      "[4.6 3.2 1.4 0.2] ->  0\n",
      "[5.3 3.7 1.5 0.2] ->  0\n",
      "[5.  3.3 1.4 0.2] ->  0\n",
      "[7.  3.2 4.7 1.4] ->  1\n",
      "[6.4 3.2 4.5 1.5] ->  1\n",
      "[6.9 3.1 4.9 1.5] ->  1\n",
      "[5.5 2.3 4.  1.3] ->  1\n",
      "[6.5 2.8 4.6 1.5] ->  1\n",
      "[5.7 2.8 4.5 1.3] ->  1\n",
      "[6.3 3.3 4.7 1.6] ->  1\n",
      "[4.9 2.4 3.3 1. ] ->  1\n",
      "[6.6 2.9 4.6 1.3] ->  1\n",
      "[5.2 2.7 3.9 1.4] ->  1\n",
      "[5.  2.  3.5 1. ] ->  1\n",
      "[5.9 3.  4.2 1.5] ->  1\n",
      "[6.  2.2 4.  1. ] ->  1\n",
      "[6.1 2.9 4.7 1.4] ->  1\n",
      "[5.6 2.9 3.6 1.3] ->  1\n",
      "[6.7 3.1 4.4 1.4] ->  1\n",
      "[5.6 3.  4.5 1.5] ->  1\n",
      "[5.8 2.7 4.1 1. ] ->  1\n",
      "[6.2 2.2 4.5 1.5] ->  1\n",
      "[5.6 2.5 3.9 1.1] ->  1\n",
      "[5.9 3.2 4.8 1.8] ->  1\n",
      "[6.1 2.8 4.  1.3] ->  1\n",
      "[6.3 2.5 4.9 1.5] ->  1\n",
      "[6.1 2.8 4.7 1.2] ->  1\n",
      "[6.4 2.9 4.3 1.3] ->  1\n",
      "[6.6 3.  4.4 1.4] ->  1\n",
      "[6.8 2.8 4.8 1.4] ->  1\n",
      "[6.7 3.  5.  1.7] ->  1\n",
      "[6.  2.9 4.5 1.5] ->  1\n",
      "[5.7 2.6 3.5 1. ] ->  1\n",
      "[5.5 2.4 3.8 1.1] ->  1\n",
      "[5.5 2.4 3.7 1. ] ->  1\n",
      "[5.8 2.7 3.9 1.2] ->  1\n",
      "[6.  2.7 5.1 1.6] ->  1\n",
      "[5.4 3.  4.5 1.5] ->  1\n",
      "[6.  3.4 4.5 1.6] ->  1\n",
      "[6.7 3.1 4.7 1.5] ->  1\n",
      "[6.3 2.3 4.4 1.3] ->  1\n",
      "[5.6 3.  4.1 1.3] ->  1\n",
      "[5.5 2.5 4.  1.3] ->  1\n",
      "[5.5 2.6 4.4 1.2] ->  1\n",
      "[6.1 3.  4.6 1.4] ->  1\n",
      "[5.8 2.6 4.  1.2] ->  1\n",
      "[5.  2.3 3.3 1. ] ->  1\n",
      "[5.6 2.7 4.2 1.3] ->  1\n",
      "[5.7 3.  4.2 1.2] ->  1\n",
      "[5.7 2.9 4.2 1.3] ->  1\n",
      "[6.2 2.9 4.3 1.3] ->  1\n",
      "[5.1 2.5 3.  1.1] ->  1\n",
      "[5.7 2.8 4.1 1.3] ->  1\n",
      "[6.3 3.3 6.  2.5] ->  2\n",
      "[5.8 2.7 5.1 1.9] ->  2\n",
      "[7.1 3.  5.9 2.1] ->  2\n",
      "[6.3 2.9 5.6 1.8] ->  2\n",
      "[6.5 3.  5.8 2.2] ->  2\n",
      "[7.6 3.  6.6 2.1] ->  2\n",
      "[4.9 2.5 4.5 1.7] ->  2\n",
      "[7.3 2.9 6.3 1.8] ->  2\n",
      "[6.7 2.5 5.8 1.8] ->  2\n",
      "[7.2 3.6 6.1 2.5] ->  2\n",
      "[6.5 3.2 5.1 2. ] ->  2\n",
      "[6.4 2.7 5.3 1.9] ->  2\n",
      "[6.8 3.  5.5 2.1] ->  2\n",
      "[5.7 2.5 5.  2. ] ->  2\n",
      "[5.8 2.8 5.1 2.4] ->  2\n",
      "[6.4 3.2 5.3 2.3] ->  2\n",
      "[6.5 3.  5.5 1.8] ->  2\n",
      "[7.7 3.8 6.7 2.2] ->  2\n",
      "[7.7 2.6 6.9 2.3] ->  2\n",
      "[6.  2.2 5.  1.5] ->  2\n",
      "[6.9 3.2 5.7 2.3] ->  2\n",
      "[5.6 2.8 4.9 2. ] ->  2\n",
      "[7.7 2.8 6.7 2. ] ->  2\n",
      "[6.3 2.7 4.9 1.8] ->  2\n",
      "[6.7 3.3 5.7 2.1] ->  2\n",
      "[7.2 3.2 6.  1.8] ->  2\n",
      "[6.2 2.8 4.8 1.8] ->  2\n",
      "[6.1 3.  4.9 1.8] ->  2\n",
      "[6.4 2.8 5.6 2.1] ->  2\n",
      "[7.2 3.  5.8 1.6] ->  2\n",
      "[7.4 2.8 6.1 1.9] ->  2\n",
      "[7.9 3.8 6.4 2. ] ->  2\n",
      "[6.4 2.8 5.6 2.2] ->  2\n",
      "[6.3 2.8 5.1 1.5] ->  2\n",
      "[6.1 2.6 5.6 1.4] ->  2\n",
      "[7.7 3.  6.1 2.3] ->  2\n",
      "[6.3 3.4 5.6 2.4] ->  2\n",
      "[6.4 3.1 5.5 1.8] ->  2\n",
      "[6.  3.  4.8 1.8] ->  2\n",
      "[6.9 3.1 5.4 2.1] ->  2\n",
      "[6.7 3.1 5.6 2.4] ->  2\n",
      "[6.9 3.1 5.1 2.3] ->  2\n",
      "[5.8 2.7 5.1 1.9] ->  2\n",
      "[6.8 3.2 5.9 2.3] ->  2\n",
      "[6.7 3.3 5.7 2.5] ->  2\n",
      "[6.7 3.  5.2 2.3] ->  2\n",
      "[6.3 2.5 5.  1.9] ->  2\n",
      "[6.5 3.  5.2 2. ] ->  2\n",
      "[6.2 3.4 5.4 2.3] ->  2\n",
      "[5.9 3.  5.1 1.8] ->  2\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(\"data  + target\")\n",
    "for i in range (iris.data.shape[0]):\n",
    "    print(x[i] ,\"-> \", y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split this data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. test_size = 0.2 shows that we are dividing the dataset into 80 - 20 pattern.\n",
    "2. random_state = 42 , will shuffle the data while dividing, in same order each time we run it again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assign variables for each hyperparameter to make fine tuning the hyperparameters easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Early Stopping\n",
    "1. Early Stopping is used so that our model would not over fit\n",
    "2. We no. of epochs are high, there is a change that our model overfits, this can be checked, when our model loss decreases but validation loss increases at some point\n",
    "3. We avoid this my stopping the training, we we get our first validation loss increase\n",
    "4. But when the increase in validation loss is significantly low, we let some epochs slide by.\n",
    "5. This is donr by adding the 'pateince' argument in Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaination for keras.Sequential\n",
    "1. We have added 2 hidden layers of size 50.\n",
    "2. Activation function used - 'relu', the activation function 'sigmoid' can also be used\n",
    "3. In the output layer, we have used 'softmax' activation function.\n",
    "4. The softmax activation function gives us the probability of a output being in any class, this value lies between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaination for compile\n",
    "1. We used optimizer 'adam' , it is latest optimizer widely used by professionals.\n",
    "2. reason- it takes into account that our model increases the learning rate in 1st part of training and we reach the global minimum the learning rate decrease will lower.\n",
    "3. also, 'adam' takes into account for 'momentum', as such our model does not reach a local minimum, it reaches to global minimum.\n",
    "4. loss function used is sparse categorial crossentropy, when we are dealing with classification problem, it is always better to rely on the concept of cross entropy rather that L2 norm, etc. cross entropy is favoured because, when there is huge difference between actual and predicted values during training phase, the cross entropy gives huge value and similarly low for lower difference.\n",
    "5. The argument metrics defines what we want to be displayed during training process, we have set it to display 'accuracy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 185ms/step - loss: 1.1339 - accuracy: 0.3229 - val_loss: 0.8836 - val_accuracy: 0.7500\n",
      "Epoch 2/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.9805 - accuracy: 0.5208 - val_loss: 0.8723 - val_accuracy: 0.4583\n",
      "Epoch 3/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.9047 - accuracy: 0.3958 - val_loss: 0.8550 - val_accuracy: 0.5000\n",
      "Epoch 4/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.8486 - accuracy: 0.7500 - val_loss: 0.8140 - val_accuracy: 0.9583\n",
      "Epoch 5/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.8085 - accuracy: 0.9375 - val_loss: 0.7577 - val_accuracy: 0.9583\n",
      "Epoch 6/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.7722 - accuracy: 0.8333 - val_loss: 0.7148 - val_accuracy: 0.8750\n",
      "Epoch 7/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.7338 - accuracy: 0.8333 - val_loss: 0.6807 - val_accuracy: 0.9583\n",
      "Epoch 8/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6948 - accuracy: 0.8438 - val_loss: 0.6444 - val_accuracy: 0.9583\n",
      "Epoch 9/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.6570 - accuracy: 0.9062 - val_loss: 0.6302 - val_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.6268 - accuracy: 0.9583 - val_loss: 0.6247 - val_accuracy: 0.9583\n",
      "Epoch 11/100\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 0.5939 - accuracy: 0.9479 - val_loss: 0.5951 - val_accuracy: 0.9583\n",
      "Epoch 12/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.5627 - accuracy: 0.9688 - val_loss: 0.5551 - val_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.5389 - accuracy: 0.9375 - val_loss: 0.5220 - val_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.5166 - accuracy: 0.9062 - val_loss: 0.5050 - val_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.4960 - accuracy: 0.9271 - val_loss: 0.5110 - val_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.4732 - accuracy: 0.9688 - val_loss: 0.5029 - val_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.4569 - accuracy: 0.9688 - val_loss: 0.4906 - val_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.4464 - accuracy: 0.9688 - val_loss: 0.4620 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.4280 - accuracy: 0.9688 - val_loss: 0.4580 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.4161 - accuracy: 0.9583 - val_loss: 0.4535 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.4035 - accuracy: 0.9688 - val_loss: 0.4333 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 0.3927 - accuracy: 0.9792 - val_loss: 0.4220 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.3822 - accuracy: 0.9688 - val_loss: 0.4078 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.3734 - accuracy: 0.9688 - val_loss: 0.3988 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.3654 - accuracy: 0.9792 - val_loss: 0.4050 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.3558 - accuracy: 0.9583 - val_loss: 0.3948 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.3467 - accuracy: 0.9688 - val_loss: 0.3779 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.3391 - accuracy: 0.9792 - val_loss: 0.3672 - val_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.3321 - accuracy: 0.9688 - val_loss: 0.3536 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.3248 - accuracy: 0.9792 - val_loss: 0.3496 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.3173 - accuracy: 0.9792 - val_loss: 0.3524 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.3115 - accuracy: 0.9479 - val_loss: 0.3535 - val_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.3038 - accuracy: 0.9583 - val_loss: 0.3300 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.2961 - accuracy: 0.9792 - val_loss: 0.3169 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.2923 - accuracy: 0.9792 - val_loss: 0.3037 - val_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.2840 - accuracy: 0.9792 - val_loss: 0.3112 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.2787 - accuracy: 0.9688 - val_loss: 0.3046 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.2735 - accuracy: 0.9583 - val_loss: 0.3050 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.2679 - accuracy: 0.9688 - val_loss: 0.2784 - val_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.2602 - accuracy: 0.9792 - val_loss: 0.2695 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.2559 - accuracy: 0.9792 - val_loss: 0.2770 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.2522 - accuracy: 0.9688 - val_loss: 0.2763 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 0.2422 - accuracy: 0.9688 - val_loss: 0.2479 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.2383 - accuracy: 0.9792 - val_loss: 0.2405 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.2335 - accuracy: 0.9792 - val_loss: 0.2287 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.2295 - accuracy: 0.9688 - val_loss: 0.2394 - val_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "3/3 [==============================] - 0s 39ms/step - loss: 0.2222 - accuracy: 0.9688 - val_loss: 0.2353 - val_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.2174 - accuracy: 0.9688 - val_loss: 0.2270 - val_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2129 - accuracy: 0.9688 - val_loss: 0.2091 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.2083 - accuracy: 0.9792 - val_loss: 0.2058 - val_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.2069 - accuracy: 0.9792 - val_loss: 0.1958 - val_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1991 - accuracy: 0.9792 - val_loss: 0.2025 - val_accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.1958 - accuracy: 0.9688 - val_loss: 0.2102 - val_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.1939 - accuracy: 0.9688 - val_loss: 0.1900 - val_accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.1872 - accuracy: 0.9688 - val_loss: 0.1795 - val_accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1865 - accuracy: 0.9688 - val_loss: 0.1817 - val_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.1800 - accuracy: 0.9688 - val_loss: 0.1705 - val_accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.1770 - accuracy: 0.9792 - val_loss: 0.1649 - val_accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.1744 - accuracy: 0.9792 - val_loss: 0.1685 - val_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.1705 - accuracy: 0.9688 - val_loss: 0.1600 - val_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "3/3 [==============================] - 0s 28ms/step - loss: 0.1677 - accuracy: 0.9688 - val_loss: 0.1556 - val_accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.1648 - accuracy: 0.9688 - val_loss: 0.1538 - val_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.1612 - accuracy: 0.9688 - val_loss: 0.1500 - val_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1587 - accuracy: 0.9688 - val_loss: 0.1448 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1570 - accuracy: 0.9688 - val_loss: 0.1397 - val_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 0.1610 - accuracy: 0.9583 - val_loss: 0.1519 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1510 - accuracy: 0.9688 - val_loss: 0.1327 - val_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.1487 - accuracy: 0.9792 - val_loss: 0.1248 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1483 - accuracy: 0.9792 - val_loss: 0.1226 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.1480 - accuracy: 0.9688 - val_loss: 0.1410 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.1436 - accuracy: 0.9583 - val_loss: 0.1327 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.1400 - accuracy: 0.9688 - val_loss: 0.1208 - val_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1379 - accuracy: 0.9688 - val_loss: 0.1097 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.1384 - accuracy: 0.9792 - val_loss: 0.1139 - val_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1353 - accuracy: 0.9792 - val_loss: 0.1169 - val_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.1344 - accuracy: 0.9688 - val_loss: 0.1136 - val_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1316 - accuracy: 0.9583 - val_loss: 0.1183 - val_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.1312 - accuracy: 0.9583 - val_loss: 0.1142 - val_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.1269 - accuracy: 0.9688 - val_loss: 0.1021 - val_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.1285 - accuracy: 0.9792 - val_loss: 0.0937 - val_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1262 - accuracy: 0.9792 - val_loss: 0.0988 - val_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.1233 - accuracy: 0.9688 - val_loss: 0.1132 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.1259 - accuracy: 0.9583 - val_loss: 0.1145 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.1215 - accuracy: 0.9583 - val_loss: 0.0946 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.1220 - accuracy: 0.9792 - val_loss: 0.0832 - val_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "3/3 [==============================] - 0s 30ms/step - loss: 0.1210 - accuracy: 0.9792 - val_loss: 0.0881 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.1167 - accuracy: 0.9792 - val_loss: 0.0996 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.1162 - accuracy: 0.9583 - val_loss: 0.1030 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.1158 - accuracy: 0.9583 - val_loss: 0.0946 - val_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "3/3 [==============================] - 0s 51ms/step - loss: 0.1147 - accuracy: 0.9688 - val_loss: 0.0850 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1129 - accuracy: 0.9792 - val_loss: 0.0831 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.1119 - accuracy: 0.9792 - val_loss: 0.0840 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "3/3 [==============================] - 0s 27ms/step - loss: 0.1120 - accuracy: 0.9583 - val_loss: 0.0950 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.1102 - accuracy: 0.9583 - val_loss: 0.0875 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1104 - accuracy: 0.9688 - val_loss: 0.0764 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "3/3 [==============================] - 0s 29ms/step - loss: 0.1080 - accuracy: 0.9792 - val_loss: 0.0787 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1074 - accuracy: 0.9792 - val_loss: 0.0795 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.1093 - accuracy: 0.9583 - val_loss: 0.0927 - val_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "3/3 [==============================] - 0s 26ms/step - loss: 0.1079 - accuracy: 0.9583 - val_loss: 0.0764 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "3/3 [==============================] - 0s 32ms/step - loss: 0.1054 - accuracy: 0.9792 - val_loss: 0.0714 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23938f4fc10>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 4\n",
    "hidden_layer_1 = 50\n",
    "hidden_layer_2 = 50\n",
    "output_size = 3\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=7)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                                tf.keras.layers.Dense(hidden_layer_1 , activation= 'relu'),\n",
    "                                tf.keras.layers.Dense(hidden_layer_2 , activation= 'relu'),\n",
    "                                tf.keras.layers.Dense(output_size , activation='softmax')\n",
    "                            ])\n",
    "\n",
    "model.compile( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy', metrics= ['accuracy'])\n",
    "\n",
    "Epochs = 100\n",
    "Batch_Size = 32\n",
    "Validation_Split = 0.2 \n",
    "\n",
    "\n",
    "model.fit(x_train, y_train, epochs = Epochs, batch_size=Batch_Size, validation_split=Validation_Split,callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1284 - accuracy: 0.9667\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We Got Accuracy of 96.67 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viva Questions\n",
    "\n",
    "1. How many parameters are there in your model?\n",
    "- We know that Parameters = (input_size + 1) * output_size\n",
    "- Dense Layer 1:\n",
    "\n",
    "- Input size: input_size = 4\n",
    "Output size: hidden_layer_1 = 50\n",
    "Parameters = (4 + 1) * 50 = 250 parameters\n",
    "-Dense Layer 2:\n",
    "\n",
    "- Input size: hidden_layer_1 = 50 (output from the previous layer)\n",
    "Output size: hidden_layer_2 = 50\n",
    "Parameters = (50 + 1) * 50 = 2550 parameters\n",
    "- Dense Layer 3 (Output Layer):\n",
    "\n",
    "- Input size: hidden_layer_2 = 50 (output from the previous layer)\n",
    "Output size: output_size = 3.\n",
    "Parameters = (50 + 1) * 3 = 153 parameters.\n",
    "- Total Parameters = Parameters in Dense Layer 1 + Parameters in Dense Layer 2 + Parameters in Dense Layer 3\n",
    "Total Parameters = 250 + 2550 + 153 = 2953 parameters\n",
    "\n",
    "\n",
    "2. Modify your model for nonlinear classification?\n",
    "- model = tf.keras.Sequential([\n",
    "-                               tf.keras.layers.Dense(hidden_layer_1 , activation= 'sigmoid'),\n",
    "                                tf.keras.layers.Dense(hidden_layer_2 , activation= 'sigmoid'),\n",
    "                                tf.keras.layers.Dense(output_size , activation='softmax')\n",
    "                            ])\n",
    "- We use sigmoid activation function to bring non linearity into the model\n",
    "    \n",
    "\n",
    "    \n",
    "3. What is the accuracy of your model? How can you improve it?\n",
    "- Accuracy of 96.67%, We can improve this fine tuning the hyperparamters like hidden layer size, no. of hidden layers in the model, changing and experimenting various activation functions, batch size, 'pateince' argument of early stopping\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-TF2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
