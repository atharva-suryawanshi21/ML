{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0330bcd0",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64581057",
   "metadata": {},
   "source": [
    "### The Basics of what we are dealing with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da61e92f",
   "metadata": {},
   "source": [
    "1. we have data of an audiobook app, where each customer has atleast made a single purchase.\n",
    "2. we want to create a machine learning algorithm that can predict if a customer will buy again.\n",
    "3. reason:- the company must not spent resources on the customers that are unlikely to come back\n",
    "4. columns containing, book length, avg book length, price and its avg, review, minutes listened, etc\n",
    "5. column review -> we have value only for those who left a review. We substitute all missing values with average review.\n",
    "6. Data taken for 2 years, and then the targets (whether the person converted )are noted in the span of 6 months, so the data is of 2.5 years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9829f591",
   "metadata": {},
   "source": [
    "### The Data : Columns explained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d71e28",
   "metadata": {},
   "source": [
    "1. Customer ID\n",
    "2. Book Length overall (in minutes)\n",
    "3. Book Length avg (in minutes)\n",
    "4. price overall\n",
    "5. price average\n",
    "6. review (1 if left a review else 0)\n",
    "7. review out of 10\n",
    "8. Minutes listened\n",
    "9. completion (Book length / minutes listened)\n",
    "9. Support request (forgot password, etc): shows in spite of such troublesome work, the customer sticked with us, also, it may happen that the customer left the platform due these troublesome works.\n",
    "10. last visited minus purchase date( higher the number the more regular the person is)\n",
    "\n",
    "11. Targets ( 0 if not converted else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd97307",
   "metadata": {},
   "source": [
    "### Since this is a real life data we need to preprocess it "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d5b1f3",
   "metadata": {},
   "source": [
    "1. Balance the dataset\n",
    "2. Divide the dataset in training, validation and test\n",
    "3. Save the data in a tensor friendly format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e6f6ee",
   "metadata": {},
   "source": [
    "### What is Balancing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e5bd4f",
   "metadata": {},
   "source": [
    "Take an example: If our data has 90% of data of cats and remaining of dogs, since our ML algoritm tries to optimize the loss, it quickly realises that if so many targets are cats then the output is more likely cats, so it comes with same prediction all the time- cats\n",
    "1. The initial probability of picking a photo of some class as a prior\n",
    "2. In above example the priors are 0.9 for cats and 0.1 for dogs.\n",
    "3. the prior must always be close to 0.5 in case of 2 classes, so in case of 3 classes it must be 0.33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b87668",
   "metadata": {},
   "source": [
    "In our audiobook data, most of the customers did not convert back, hence we must have equal number of customer those who did and did not convert back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7417dc",
   "metadata": {},
   "source": [
    "# Preprocessing begins "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c144a1",
   "metadata": {},
   "source": [
    "### Extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fe5c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "raw_csv_data = np.loadtxt('Audiobooks_data.csv', delimiter =',')\n",
    "unscaled_inputs_all = raw_csv_data[:,1:-1] # 1-> second column till -1 -> last column excluded\n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbad2302",
   "metadata": {},
   "source": [
    "we dont need customer id, i.e. the first column as it does not bring any value to us. So we remove the tragets and ID to produnce inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c431a5db",
   "metadata": {},
   "source": [
    "### Shuffle the data\n",
    "before balancing the data, shuffling ensures that random records of 0 targets are eliminated, hence shuffling must be done before it. For e.g. what if the data is arranged in the order of the data, so while batching this may confuse the SGD when we average the loss across the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5363d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(unscaled_inputs_all.shape[0])\n",
    "seed =100\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "unscaled_inputs_all = unscaled_inputs_all[shuffled_indices]\n",
    "targets_all = targets_all[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703bdc32",
   "metadata": {},
   "source": [
    "### Balance the data\n",
    "1. we count the no. of targets that are '1' \n",
    "2. we keep many 0s as there are 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beb4e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_of_1s= np.sum(targets_all == 1)\n",
    "count_of_0s = 0 # currently 0\n",
    "indices_to_remove = [] # we want it to be list or tuple hence we put empty brackets\n",
    "\n",
    "for i in range(targets_all.shape[0]):  # shape on the zero axis is the length of the vector\n",
    "    if targets_all[i]==0:\n",
    "        count_of_0s += 1\n",
    "        if count_of_0s > count_of_1s:\n",
    "            indices_to_remove.append(i)     \n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all,indices_to_remove,axis =0)\n",
    "targets_equal_priors = np.delete(targets_all,indices_to_remove,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47706fed",
   "metadata": {},
   "source": [
    "### Standardize the inputs\n",
    "If not done, it reduces the accuracy of model by 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76223a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a2ab84",
   "metadata": {},
   "source": [
    "## Important concept\n",
    "we shuffled the data, and then balanced it. When we balance it may happen that one category of class may accumulate in one of the set (train validation test). So we reshuffle the data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22cee5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the data was collected it was actually arranged by date\n",
    "# Shuffle the indices of the data, so the data is not arranged in any way when we feed it.\n",
    "# Since we will be batching, we want the data to be as randomly spread out as possible\n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "# Use the shuffled indices to shuffle the inputs and targets.\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f92149",
   "metadata": {},
   "source": [
    "### Split the data into training, validation and test\n",
    "we will be using 80-10-10 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "981b290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1772.0 3579 0.4951103660240291\n",
      "237.0 447 0.5302013422818792\n",
      "228.0 448 0.5089285714285714\n"
     ]
    }
   ],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "train_count = int(0.8 * samples_count)\n",
    "validation_count = int(0.1 * samples_count)\n",
    "test_count = samples_count - train_count - validation_count\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_count]\n",
    "train_targets = shuffled_targets[:train_count]\n",
    "\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_count:train_count+validation_count]\n",
    "validation_targets = shuffled_targets[train_count:train_count+validation_count]\n",
    "\n",
    "test_inputs = shuffled_inputs[train_count+validation_count:]\n",
    "test_targets = shuffled_targets[train_count+validation_count:]\n",
    "\n",
    "print(np.sum(train_targets),train_count,np.sum(train_targets)/train_count)\n",
    "print(np.sum(validation_targets),validation_count,np.sum(validation_targets)/validation_count)\n",
    "print(np.sum(test_targets),test_count,np.sum(test_targets)/test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aac7a3",
   "metadata": {},
   "source": [
    "we notice that we got approx 50% priors in three cases. (50% -55% also acceptable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef11396",
   "metadata": {},
   "source": [
    "## Now we save our data into npz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fa4b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobook_train_data',inputs = train_inputs,targets = train_targets)\n",
    "np.savez('Audiobook_validation_data',inputs = validation_inputs,targets = validation_targets)\n",
    "np.savez('Audiobook_test_data',inputs = test_inputs,targets = test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f39a096",
   "metadata": {},
   "source": [
    "#### note:\n",
    "you can write any word like cat dog, input, inputss, etc instead of 'inputs' in the savez function, same goes for 'targets'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
