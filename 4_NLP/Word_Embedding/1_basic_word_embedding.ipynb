{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Word embedding is a core concept in Natural Language Processing (NLP) where words are represented as numerical vectors.\n",
    "2. This technique is crucial for NLP tasks, enabling machines to understand word meanings and relationships.\n",
    "Significance of Word Embedding:\n",
    "\n",
    "3. Word embedding captures semantic relationships, making it a key component in NLP applications like sentiment analysis and machine translation.\n",
    "4. It reduces dimensionality, improving model performance and facilitating the processing of large text corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = {\n",
    "    'too good',\n",
    "    'very nice food',\n",
    "    'amazing restaurant',\n",
    "    'very bad',\n",
    "    \n",
    "    'just loved it',\n",
    "    'will go again',\n",
    "    'horrible food',\n",
    "    'never go there',\n",
    "    'poor service',\n",
    "    'poor quality',\n",
    "    'very nice',\n",
    "    'needs improvement'\n",
    "}\n",
    "sentiments = np.array([1,1,1,0,1,1,0,0,0,0,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert into one hot vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 'encoded_review' will contain lists of integers, where each integer corresponds to a word in the input text. \n",
    "2. The vocabulary size is set to 30, and the function assigns unique integers to the words in the input text within that vocabulary size constraint. \n",
    "3. The specific integer assigned to each word is determined by the hashing function used by one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[34, 1],\n",
       " [36, 2],\n",
       " [5, 9, 10],\n",
       " [23, 9, 30],\n",
       " [26, 32, 43],\n",
       " [34, 1, 2],\n",
       " [8, 48],\n",
       " [34, 10],\n",
       " [10, 31],\n",
       " [10, 1],\n",
       " [5, 48],\n",
       " [14, 46]]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "\n",
    "vocabulary_size = 50\n",
    "encoded_review = [one_hot(sentence, vocabulary_size) for sentence in reviews]\n",
    "encoded_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "1. Some sentences are 3 word long and some ar 4.\n",
    "2. So we need padding to have uniform size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[34,  1,  0],\n",
       "       [36,  2,  0],\n",
       "       [ 5,  9, 10],\n",
       "       [23,  9, 30],\n",
       "       [26, 32, 43],\n",
       "       [34,  1,  2],\n",
       "       [ 8, 48,  0],\n",
       "       [34, 10,  0],\n",
       "       [10, 31,  0],\n",
       "       [10,  1,  0],\n",
       "       [ 5, 48,  0],\n",
       "       [14, 46,  0]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_length = 3\n",
    "padded_reviews = pad_sequences(encoded_review, maxlen = max_length, padding = 'post')\n",
    "padded_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 3, 4)              200       \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 12)                0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1)                 13        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213\n",
      "Trainable params: 213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedded_vector_size = 4\n",
    "train = padded_reviews\n",
    "targets = sentiments\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Embedding(vocabulary_size, embedded_vector_size, input_length = max_length, name = 'embedding'),\n",
    "    Flatten(),\n",
    "    Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "model.compile('adam', loss = 'binary_crossentropy', metrics =['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 1s - loss: 0.6927 - accuracy: 0.5833 - 610ms/epoch - 610ms/step\n",
      "Epoch 2/30\n",
      "1/1 - 0s - loss: 0.6914 - accuracy: 0.5833 - 3ms/epoch - 3ms/step\n",
      "Epoch 3/30\n",
      "1/1 - 0s - loss: 0.6901 - accuracy: 0.5833 - 2ms/epoch - 2ms/step\n",
      "Epoch 4/30\n",
      "1/1 - 0s - loss: 0.6888 - accuracy: 0.5833 - 2ms/epoch - 2ms/step\n",
      "Epoch 5/30\n",
      "1/1 - 0s - loss: 0.6876 - accuracy: 0.5833 - 3ms/epoch - 3ms/step\n",
      "Epoch 6/30\n",
      "1/1 - 0s - loss: 0.6863 - accuracy: 0.5833 - 2ms/epoch - 2ms/step\n",
      "Epoch 7/30\n",
      "1/1 - 0s - loss: 0.6850 - accuracy: 0.6667 - 3ms/epoch - 3ms/step\n",
      "Epoch 8/30\n",
      "1/1 - 0s - loss: 0.6837 - accuracy: 0.6667 - 999us/epoch - 999us/step\n",
      "Epoch 9/30\n",
      "1/1 - 0s - loss: 0.6824 - accuracy: 0.6667 - 2ms/epoch - 2ms/step\n",
      "Epoch 10/30\n",
      "1/1 - 0s - loss: 0.6811 - accuracy: 0.6667 - 999us/epoch - 999us/step\n",
      "Epoch 11/30\n",
      "1/1 - 0s - loss: 0.6799 - accuracy: 0.6667 - 3ms/epoch - 3ms/step\n",
      "Epoch 12/30\n",
      "1/1 - 0s - loss: 0.6786 - accuracy: 0.7500 - 3ms/epoch - 3ms/step\n",
      "Epoch 13/30\n",
      "1/1 - 0s - loss: 0.6773 - accuracy: 0.7500 - 3ms/epoch - 3ms/step\n",
      "Epoch 14/30\n",
      "1/1 - 0s - loss: 0.6760 - accuracy: 0.7500 - 2ms/epoch - 2ms/step\n",
      "Epoch 15/30\n",
      "1/1 - 0s - loss: 0.6747 - accuracy: 0.7500 - 2ms/epoch - 2ms/step\n",
      "Epoch 16/30\n",
      "1/1 - 0s - loss: 0.6735 - accuracy: 0.7500 - 2ms/epoch - 2ms/step\n",
      "Epoch 17/30\n",
      "1/1 - 0s - loss: 0.6722 - accuracy: 0.7500 - 998us/epoch - 998us/step\n",
      "Epoch 18/30\n",
      "1/1 - 0s - loss: 0.6709 - accuracy: 0.7500 - 2ms/epoch - 2ms/step\n",
      "Epoch 19/30\n",
      "1/1 - 0s - loss: 0.6696 - accuracy: 0.8333 - 2ms/epoch - 2ms/step\n",
      "Epoch 20/30\n",
      "1/1 - 0s - loss: 0.6683 - accuracy: 0.8333 - 3ms/epoch - 3ms/step\n",
      "Epoch 21/30\n",
      "1/1 - 0s - loss: 0.6670 - accuracy: 0.9167 - 2ms/epoch - 2ms/step\n",
      "Epoch 22/30\n",
      "1/1 - 0s - loss: 0.6657 - accuracy: 0.9167 - 2ms/epoch - 2ms/step\n",
      "Epoch 23/30\n",
      "1/1 - 0s - loss: 0.6644 - accuracy: 0.9167 - 2ms/epoch - 2ms/step\n",
      "Epoch 24/30\n",
      "1/1 - 0s - loss: 0.6631 - accuracy: 0.9167 - 2ms/epoch - 2ms/step\n",
      "Epoch 25/30\n",
      "1/1 - 0s - loss: 0.6617 - accuracy: 0.9167 - 3ms/epoch - 3ms/step\n",
      "Epoch 26/30\n",
      "1/1 - 0s - loss: 0.6604 - accuracy: 0.9167 - 3ms/epoch - 3ms/step\n",
      "Epoch 27/30\n",
      "1/1 - 0s - loss: 0.6591 - accuracy: 0.9167 - 2ms/epoch - 2ms/step\n",
      "Epoch 28/30\n",
      "1/1 - 0s - loss: 0.6577 - accuracy: 0.9167 - 3ms/epoch - 3ms/step\n",
      "Epoch 29/30\n",
      "1/1 - 0s - loss: 0.6564 - accuracy: 0.9167 - 2ms/epoch - 2ms/step\n",
      "Epoch 30/30\n",
      "1/1 - 0s - loss: 0.6551 - accuracy: 0.9167 - 4ms/epoch - 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x267fb813b50>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, targets, epochs =30, verbose =2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 208ms/step - loss: 0.6537 - accuracy: 0.9167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6536964178085327, 0.9166666865348816]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(train,targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note:\n",
    "1. This is a fake problem, we are more interested in the word embedding which we will do now\n",
    "2. we have that data in the Embedding layer- \"embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05637718, 0.07565457, 0.06916998, 0.01669856], dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02278753,  0.0499354 ,  0.0567701 ,  0.07229727], dtype=float32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[48]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1-> good\n",
    "- 48-> nice \n",
    "- from one_hot \n",
    "- even though good and nice have same meaning the vector are different as we had limited dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
