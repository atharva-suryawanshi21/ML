{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5197e0a7",
   "metadata": {},
   "source": [
    "# MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db27fba",
   "metadata": {},
   "source": [
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image). \n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes. \n",
    "\n",
    "Our goal would be to build a neural network with 2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "15204e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "# these datasets will be stored in C:\\Users\\*USERNAME*\\tensorflow_datasets\\...\n",
    "# the first time you download a dataset, it is stored in the respective folder \n",
    "# every other time, it is automatically loading the copy on your computer\n",
    "\n",
    "#satha is my username"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d877926b",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f96ca4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "mnist_dataset,mnist_info = tfds.load(name='mnist',with_info = True, as_supervised = True)\n",
    "# as_supervised will load the data in a 2 tuple structure [inputs,outputs]\n",
    "# with_info provides us with tuple containing information about version, features and number of samples in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0edfef0",
   "metadata": {},
   "source": [
    "The mnist dataset has only train(60,000) and test(10,000) data set and no validation data set. \n",
    "So we take a percentage of train data set as validation data set|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "35502963",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train , mnist_test = mnist_dataset['train'],  mnist_dataset['test']\n",
    "\n",
    "\n",
    "no_of_train_sample = mnist_info.splits['train'].num_examples\n",
    "no_of_validation_sample = 0.1 * no_of_train_sample\n",
    "no_of_validation_sample = tf.cast(no_of_validation_sample,tf.int64)\n",
    "# The split may not always provide an integer value hence a decimal number may bring error while splitting \n",
    "# so we cast it to an integer\n",
    "\n",
    "no_of_test_sample = tf.cast(mnist_info.splits['test'].num_examples,tf.int64)\n",
    "# Notice that we merged two lines of code into one, i.e. taking split value and casting it into integer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d85c1b3",
   "metadata": {},
   "source": [
    "Now we scale our data to make it numerically stable , inputs between 0 and 1.\n",
    "There is a function 'dataset.map(*func*)' , it applies custom transformation to given dataset. It takes function as input.\n",
    "Note:- the input function must take image and label as input and return the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "adc5e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image,label):\n",
    "    image = tf.cast(image,tf.float32)/255.     # '.' ensures floating value at end\n",
    "    return image,label\n",
    "\n",
    "scaled_train_and_validation = mnist_train.map(scale)\n",
    "scaled_test = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6119f872",
   "metadata": {},
   "source": [
    "#### Explaination of above code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0312089d",
   "metadata": {},
   "source": [
    "Our data is of the form of 28 *28 matrix of pixels, each containing value between 0 and 255 (representing 256 shades of grey). Here 0 being absolute black and 255 being absolute white. Hence we divide with 255. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb16dc75",
   "metadata": {},
   "source": [
    "#### Why is Shuffling needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf57439b",
   "metadata": {},
   "source": [
    "Our dataset may be in an ascending/decending order, so while making batches this may hinder learning as data with same targets get into one batch. So we must uniformly shuffle the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e319ea62",
   "metadata": {},
   "source": [
    "When we are dealing with enormous dataset, we can't shuffle the whole data at once (as we cannot load entire data in the memory). To solve this problem we must define value of 'buffer_size' such that tensorflow can take that value of dataset and shuffle them, again take the same value of dataset and shuffle again and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4b1d5798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buffersize = 1 -> no shuffling takes place\n",
    "#buffersize >= no. of samples  -> shuffled at once , uniformly\n",
    "#1 < buffersize < no. of samples -> optimal computing power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f35563e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 10_000\n",
    "\n",
    "shuffled_train_and_validation = scaled_train_and_validation.shuffle(buffer_size)\n",
    "\n",
    "# now we split the training data to get validation dataset\n",
    "validation_data = shuffled_train_and_validation.take(no_of_validation_sample)\n",
    "# takes the first 'no_of_validation_sample' from the dataset\n",
    "\n",
    "# now we need training data without containing any validation data\n",
    "train_data = shuffled_train_and_validation.skip(no_of_validation_sample)\n",
    "# skips the first 'no_of_validation_sample' from the dataset and selects remaining data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedc949b",
   "metadata": {},
   "source": [
    "To train  our model we will be using mini batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "27822b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 1 -> Stochastic Gradient Descent (no use)\n",
    "# batch_size = no. of samples -> Gradient descent(most accurate but time consuming)\n",
    "# 1 < batch_size < no. of samples  -> Mini batch(professionally known as Stochastic Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "25851c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# we use 'dataset.batch(batch_size)' to combine the consecutive elements of a dataset into batches\n",
    "# we also override the train_data - as train_data without batch not required\n",
    "\n",
    "train_data = train_data.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32f5be6",
   "metadata": {},
   "source": [
    "#### what about validation data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19e1ad",
   "metadata": {},
   "source": [
    "When batching we find the AVERAGE loss but during validation and testing we want the exact values so we take all the data at once. We won't be backpropagating on validation data but only forward propagation, so we dont really need to batch it BUT..... Our model expect them to be in batches(both validation and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "612e26d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = validation_data.batch(no_of_validation_sample)\n",
    "test_data = scaled_test.batch(no_of_test_sample)\n",
    "# here both are of single batch\n",
    "# new column created in our tensor, indicating that the model should take entire dataset at once when it utilizes it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5330cfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_inputs, validation_targets = next(iter(validation_data))\n",
    "# our validation data must have the same shape and object properties as the train and test data.\n",
    "# MNIST data is iterable and in 2 tuple format(as_supervised = True)\n",
    "# so we must extract and convert the validation inputs and targets appropriately\n",
    "\n",
    "# iter() creates an object which can be iterated one element at a time\n",
    "# next() loads the next batch, as there is only one batch it will load inputs and targets\n",
    "# next() for only one batch is unnecessary but it is done to follow a consistent pattern when working with datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e29cad9",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8394b7",
   "metadata": {},
   "source": [
    "##### Understanding the initials \n",
    "We have 784 inputs in our neural networks, as we have (28 * 28) pixels converted into one vector. Also we have 10 outputs, digits 0 to 9. In this case, we take 2 hidden layers, 50 each. As the depth of neural network is a hyperparameter, we can change these values to get more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2f7e00be",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 200\n",
    "# we have taken size of all hidden layers to be same,i.e. 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "efcb540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "                               tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                               tf.keras.layers.Dense(hidden_layer_size,activation='relu'),# we are getting from first layer(input layer), to first hidden layer\n",
    "                               tf.keras.layers.Dense(hidden_layer_size,activation='tanh'),# first hidden layer to second hidden layer\n",
    "                               tf.keras.layers.Dense(output_size,activation='softmax'),# second hidden layer to output layer, for output layer we use softmax or argmax\n",
    "                            ])\n",
    "# t.keras.Sequencial() lays down the model, stacks layers\n",
    "# first layer-> input layer\n",
    "#     our data is of 28*28*1 (tensor of rank 3), so we need to flatten images into single vector\n",
    "# second layer onwards to build each concecutive layer of neural network\n",
    "#     it takes inputs, calculates dot product of the inputs and the weights and adds the bais, activation function applied too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d78caa1",
   "metadata": {},
   "source": [
    "##### Now we optimise our model and choose suitable loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d782c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "# ADAM optimizr\n",
    "#     it optimises the rate at which learning change and also checks for 'momentum'\n",
    "# Cross entropy\n",
    "#     the usual squared residual is a slow function, the cross entropy function punishes the model at wrong prediction with large values which is not the case with the prior function.\n",
    "# Metrics = ['accuracy']\n",
    "#     throughout the training and testing processes it updates us with 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583eec2b",
   "metadata": {},
   "source": [
    "##### Now we fit the data into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e6e69397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 4s - loss: 0.2554 - accuracy: 0.9246 - val_loss: 0.1212 - val_accuracy: 0.9658 - 4s/epoch - 7ms/step\n",
      "Epoch 2/5\n",
      "540/540 - 3s - loss: 0.0977 - accuracy: 0.9701 - val_loss: 0.0966 - val_accuracy: 0.9712 - 3s/epoch - 6ms/step\n",
      "Epoch 3/5\n",
      "540/540 - 3s - loss: 0.0654 - accuracy: 0.9797 - val_loss: 0.0706 - val_accuracy: 0.9795 - 3s/epoch - 6ms/step\n",
      "Epoch 4/5\n",
      "540/540 - 3s - loss: 0.0474 - accuracy: 0.9849 - val_loss: 0.0621 - val_accuracy: 0.9802 - 3s/epoch - 6ms/step\n",
      "Epoch 5/5\n",
      "540/540 - 3s - loss: 0.0356 - accuracy: 0.9885 - val_loss: 0.0433 - val_accuracy: 0.9875 - 3s/epoch - 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14c054212a0>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_epochs = 5\n",
    "# we dedicate variables to hyperparameters like input, output, buffer sizes to easily spot them when we fine tune the model\n",
    "model.fit(train_data, epochs=no_of_epochs,validation_data=(validation_inputs,validation_targets),verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9857c5",
   "metadata": {},
   "source": [
    "##### What happens inside an epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198435e1",
   "metadata": {},
   "source": [
    "1. At the beginning  of each epoch, the training loss will be set to 0.\n",
    "2. The algorithm will iterate over a present number of batches, all from train_data\n",
    "3. The weights and baises will be updated as many times as there are batches\n",
    "4. We will get value for the loss function, indicating how the training is going\n",
    "5. We will also see a training accuracy, due to the addiction of metric='[accuracy]'.\n",
    "6. At the end of the epoch, the algorithm will forward propagate the whole validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "103e8962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note:\n",
    "# validation accuracy is the true accuracy of the model\n",
    "# training accuracy is the average across batches\n",
    "# validation accuracy is of the whole validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b391fca2",
   "metadata": {},
   "source": [
    "### Playing with hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e055ea",
   "metadata": {},
   "source": [
    "1. hidden layer width = 200 -> accuracy = 98.78 %\n",
    "2. 3 hidden layers -> accuracy = 98.67%\n",
    "3. 5 hidden layers -> width=50 (accuracy = 96.88 %), width=100 (accuracy = 98.22 %), width=200 (accuracy = 98.45 %), width=300 (accuracy = 98.20 %)\n",
    "4. both hidden layer, activation function ='sigmoid'->accuracy = 97.35 %\n",
    "5. 2nd hidden layer , activation function ='tanh' -> accuracy = 97.42 %\n",
    "6. both hidden layer , activation function ='tanh' -> accuracy = 97.03 %\n",
    "7. both hidden layer , activation function ='tanh' with hidden layer size= 200 -> accuracy = 98.68 %\n",
    "8. learning rate = 0.0001 instead of 'adam' -> accuracy = 97.78 %\n",
    "9. learning rate = 0.0001 instead of 'adam' -> accuracy = 97.85%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56239606",
   "metadata": {},
   "source": [
    "#### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870e0a0a",
   "metadata": {},
   "source": [
    "1. Above is just validation accuracy, we still need to check for final test data accuracy.\n",
    "2. We train on the the training data and validate with validation data, this ensures what weigths, parameters and baises dont overfit the model\n",
    "3. once we train our first model, we fiddle with the hyperparameters (things we did when we were 'playing' with hyperparameters), i.e. we tried to find the best hyperparameters.\n",
    "4. By playing with hyperparameters we didnt find the best hyperparameters in general BUT the hyperparameters that fit out validation dataset best.\n",
    "5. By fine tuning them we were overfitting the validation dataset.\n",
    "6. Test data is the reality check for our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097c4274",
   "metadata": {},
   "source": [
    "## Testing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "200dff3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 530ms/step - loss: 0.0708 - accuracy: 0.9765\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2102ce7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.07\n",
      "Test accuracy: 97.65%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}\\nTest accuracy: {1:.2f}%'.format(test_loss,test_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a9bed1",
   "metadata": {},
   "source": [
    "# Important Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8946b8",
   "metadata": {},
   "source": [
    "1. Now we are no longer allowed to change it.\n",
    "2. If we now start to change the model after this point, the test data wont be the dataset that our model has never seen.\n",
    "3. If we had accurcy of 50 to 60 %, we may surely know that our model has overfit the data.\n",
    "4. But we got accuracy near close to validation data accuracy, hence we did not overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d00619",
   "metadata": {},
   "source": [
    "# Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
